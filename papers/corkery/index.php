<?php

$author = "corkery";
include_once('../../inc/paper-heading.php');
// print_r($paper);

?>





<div class="container">
	<div class="row my-2">
		<div class="col-12 col-lg-8 offset-lg-2">

			<?php include_once('../../inc/paper-thumbnail.php'); ?>



			<h5 class="subheading">Introduction</h5>


			<p>
				The machine learning market is expected to grow from $21.17 billion in 2022 to $209.21 billion in 2029 (“Machine Learning Market Size, Share, Growth & Trends [2022-2029]”). Advancements within the field have been responsible for some of the biggest technological developments in recent times. Self-driving cars, facial recognition softwares, and personalized recommendation systems are some of the most well-known and utilized products designed from machine learning concepts. However, the complexity of the models behind the technologies is far beyond the understanding of almost all of its users. The majority of users are unaware of the data collection process and many believe that the tracking involved in the data collection process is unavoidable. If users have a poor understanding of how these models work, the developers of the model receive less accountability and can get away with more tracking than the users are aware of. The more tracking and collection being done, the more accurate the models will perform, generating higher revenue and attracting more customers. Higher accuracy does not correlate to better-performing models however. As the amount of data being collected and processed increases so too does the risk of having some encoded bias, resulting in a higher likelihood of more radical or harmful views promoted by the system. Although these algorithms are able to process large amounts of data and solve complex problems, the amount of exposure people have to products designed with machine learning allows for results generated by these models to have a strong influence over users. Whether the influence is positive or negative, algorithms with an encoded bias within their model run the risk of promoting this bias and influencing people’s ideas, values, and decisions. But, as long as these models continue to increase profits for businesses, changes will not be made to identify and remove biases, and people, specifically minorities, will continue to be harmed by its effects.
			</p>

			<p>
				Machine learning concepts have been utilized in the majority of domains to improve efficiency and solve complex challenges, and this holds especially true in the marketing industry through online advertisements. In the past, advertisers’ focus was on TV and the radio, but today, online advertising takes up the majority of the market, and increases in digital ad revenue can be attributed to online ads built through machine learning concepts. Digital ad revenue in the US was $189 billion in 2021, about a $100 billion increase from 2017 (Gonzalez and Plane). The appeal of online advertising is that its performance is much easier to measure than TV and radio ads due to its instant feedback through user responses. User responses noted by the algorithm include clicking or tapping on an advertisement, or any actions taken past clicking on the ad such as making a purchase. Clicks have a direct correlation to money made by the advertisers and publishers. If advertisers could predict what users are likely to click on their ad, they could target their ads specifically to them, bringing in clicks and revenue (Plane). That’s what the algorithms do. Given observed user data, such as age, gender, and previously clicked on advertisements, the algorithm generates the probability of the user clicking on an advertisement for a list of products. The ones with the highest probability of being clicked on are displayed to the user.
			</p>

			<p>
				Not all users care for personalized ads, but rather are concerned about how their data is collected. For many users, they are unaware of how their data is being gathered and what is being done with it. Companies often share user data with each other and one’s activity on one platform can be shared through a string of companies to generate an advertisement on a completely different platform. Google offers paid search advertising, where companies can target their ads to users via their search queries on Google. Facebook provides advertisers with user information such as demographics from user-created content. Some appreciate the benefits that targeted advertisements bring but others have concerns around their privacy and the amount of influence these advertisements have.
			</p>

			<p>
				Research has indicated that targeted advertising is poorly understood by users. A study found that users displayed interest in receiving personalized ads, but expressed concern in their privacy, specifically with the data collection. Furthermore, many users view the tracking involved in data collection is unavoidable, and view personalized ads as consolation. As ads become more personalized, users tend to get less comfortable, but concerns surrounding the inferencing of these algorithms do decrease when the users believe the results to be accurate (Plane). Companies can openly disclose where they are getting their data from, but this tends to drop user interest in purchasing, seen in a study where interest in purchasing dropped 24% when exposed to where their data was coming from (John). This means companies have to make the choice between openness and profit, and companies will almost always choose profit. If profit is the only concern for advertisers, then harmful effects caused by their algorithms will continue to happen. For example, it was found that ads for high-paying executive jobs were shown significantly more to men than women and ads shown during search were more likely to associate stereotypically African American names than stereotypically white names with arrest record claims (Plane). Users coming across these harmful views and associations are subject to being persuaded to accept these views, even if it is subconsciously, or being exposed to hurtful ideas where they are the victim.
			</p>

			<p>
				The power these systems hold is exemplified by the YouTube recommendation algorithm and how it tends to promote extreme-right political content. As a for-profit company, YouTube does not act as an impartial third party, but rather pushes users towards their advertisements, which can unintentionally promote racist viewpoints (Bryant 86). This is the directive of their algorithm, but unintentional consequences often arise as a result. The algorithm measures its success through whether or not the suggested video is clicked, noting a connection between the video watched and the video clicked on. This is how the algorithm generates more accurate suggestions. However, Youtube automatically plays the suggested video after the initial video ends, causing inaccurate data to be collected whenever the suggested video starts playing without user consent (Bryant).
			</p>

			<p>
				As a for-profit company, Youtube’s goal has always been to make money, which mainly comes from engagement with their advertisements. An independent test found that the algorithm was just heavily centered around getting clicks on their advertisements and recommending videos that are likely to keep users on the site (Bryant). Furthermore, research was conducted by mapping over 13,000 channels including top channels from both ends of the political spectrum and popular channels in other top categories on the website such as gaming and tech, and attempting to recreate the recommendation algorithm to find out what was happening when extremist political content was being recommended. A tighter connection was found between right-leaning content and extreme far-right suggestions than with the left-leaning content, demonstrating how someone viewing moderate-right content is only a couple clicks away from viewing extremist videos. Along with this, it was noted that the algorithm makes an unlikely connection between unrelated top channels and extreme political content, meaning these channels are being recommended to a wider range of users (Bryant). Because the Youtube algorithm promotes videos that are more likely to keep you on the site, extremist content is often recommended no matter its relevance to the video being watched as these videos have proven to have high engagement.
			</p>



		</div>
	</div>
</div>


<div class="container-fluid px-0 my-2 mb-4">
	<div class="container px-0">
		<div class="row">
			<div class="col-12">
				<iframe id="iframe1" width="100%" height="860px" frameBorder="0" src="https://sneakaway-studio.github.io/mlPersuade/" scrolling="no"> </iframe>

				<figcaption class="figure-caption">


					<details>
						<summary>drawn() by <?php print $paper['author']; ?>; recoded (2024) by Yumna Ahmed</summary>


						<p>
							The topic of online surveillance has never bothered me too much. I've always had the sentiment that there are so many other people online and I don't have anything to hide, so it's not a big deal. Every so often a targeted ad would appear on some social media though and it would just feel too targeted. It is uncomfortable not knowing how someone has information on you, especially if it feels like its being done secretly. As I became aware of how data is sold and shared between companies, I actually felt more comfortable with targeted content because at least I knew how my data was being collected.
						</p>

						<p>
							However, the amount of impact these systems that produce targeted content have on their users can cause big issues and affect lots of people. If people are only receiving content and media from sources they are most likely to agree with, the risk of mass misinformation and radical ideals increases heavily. The focus of my game center’s around this, and specifically touches on the real world impacts that popular platforms can cause with their targeted content.
						</p>

						<p>
							As I developed the game, I got more and more into the actual game development and using JavaScript to customize, so it’s definitely supposed to feel more like a traditional game than usual Twine games, as I ended up completely venturing outside Twine. We had talked earlier in the year about gamification and how it’s used to make certain tasks or work more fun, to hide how terrible the work actually is. I decided to connect the idea of gamification with my project by designing simple or well-known games that have a theme around the damages these different platforms can cause. But the game does not feel serious or dark, rather it’s makes the theme fun, which is why targeted content is typically accepted because it provides the user with some sort of benefit to hide anything problematic.
						</p>

						<p>
							It was frustrating to make this game at times because the bugs started to take forever to fix once I started using languages and tools I had not used before. But it was very enjoyable exploring the many different things I could do to design my game. If I had more time, I would have added more apps/games but for now there are three to play.
						</p>

					</details>

				</figcaption>


			</div>
		</div>
	</div>
</div>



<div class="container">
	<div class="row my-2">
		<div class="col-12 col-lg-8 offset-lg-2">


			<p>
				Machine learning models often have issues with bias: bias within the data used to train the models, and the biases these models can sometimes promote as a byproduct of the data. Datasets that contain human-reported data are naturally going to contain some sort of human bias. For example, machine learning is used to determine one’s bail or parole eligibility by predicting whether or not an individual is likely to commit crimes in the future. However, the data covers arrest records, not crimes committed, which creates a bias within the model against minorities––especially for drug crimes––as they are policed at a higher rate (Chouldechova). Producing biased results is not a major concern if the biases are recognized, but more often than not these results have a major influence on decisions made by humans. The influence algorithmic predictions have over human decisions can be described by the anchoring effect, which is “when individuals assimilate their estimates to a previously considered standard” (Vaccaro). Multiple experiments have been conducted over the years to back this idea, including a study where real estate agents were asked to visit a home, review a detailed booklet about the house, and then determine the value of the house. However, for one group, the booklet contained a low asking price for the house, whereas the other group’s booklets contained a high asking price for the home. Those with the higher asking prices valued the house 41% higher than those with the lower asking price (Vaccaro). Although the use of an algorithm was not used to list the asking prices, the study highlights the impact of a predetermined result over human decision-making.
			</p>

			<p>
				Although issues related to machine learning systems have been found and reported, nothing changes if the developers of these systems ignore them. Timnit Gebru, the former co-lead of Google’s ethical AI team, wrote a paper outline the risks and harms large language processing models have. These large models grab massive amounts of data from all over the internet, in hopes of covering as many perspectives as they can, but do not filter their data well enough to remove all instances of hate and extremist ideas. Furthermore, the data being pulled from the internet is only going to be from those with access to the internet, excluding perspectives from many minority groups, causing bias within the data. Google’s search engine is an example of one of these large language models, and while entering in a query, the engine works to predict and autocomplete the rest of the query (Bender). But when the model’s data is biased, the engine might autocomplete to a harmful query, either promoting the hateful idea, encouraging the user to click on the query, or exposing those affected to the hate. It’s important that biased data is taken care of, so misinformation and hate can lose any momentum to spread.
			</p>


		</div>
	</div>
</div>







<div class="container">
	<div class="row my-2">
		<div class="col-12 col-lg-8 offset-lg-2">

			<h4>Bibliography</h4>


			<ol>

				<li>Bender, Emily M., et al. "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?." Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021.
				</li>
				<li>Bryant, Lauren Valentino. “The YouTube Algorithm and the Alt-Right Filter Bubble.” Open Information Science, vol. 4, no. 1, 8 June 2020, pp. 85–90, www.degruyter.com/document/doi/10.1515/opis-2020-0007/html, 10.1515/opis-2020-0007.
				</li>
				<li>Chouldechova, Alexandra, and Aaron Roth. “A Snapshot of the Frontiers of Fairness in Machine Learning.” Communications of the ACM, vol. 63, no. 5, 20 Apr. 2020, pp. 82–89, 10.1145/3376898.
				</li>
				<li>Gonzalez, Wendy. “Council Post: How Machine Learning Is Shaping the Future of Advertising.” Forbes, 19 Jan. 2022, www.forbes.com/sites/forbesbusinesscouncil/2022/01/18/how-machine-learning-is-shaping-the-future-of-advertising/?sh=3232d9a81361.
				</li>
				<li>John, Leslie, et al. “Targeting Ads without Creeping out Your Customers.” Harvard Business Review, 2018, hbr.org/2018/01/ads-that-dont-overstep.
				</li>
				<li>“Machine Learning Market Size, Share, Growth & Trends [2029].” Fortunebusinessinsights.com, 2022, www.fortunebusinessinsights.com/machine-learning-market-102226.
				</li>
				<li>Plane, Angelisa, et al. Exploring User Perceptions of Discrimination in Online Targeted Advertising Open Access to the Proceedings of the 26th USENIX Security Symposium Is Sponsored by USENIX Exploring User Perceptions of Discrimination in Online Targeted Advertising. 2017.</li>
				<li>Vaccaro, Michelle, and Jim Waldo. "The effects of mixing machine learning and human judgment." Communications of the ACM 62.11 (2019): 104-110.
				</li>

			</ol>


		</div>
	</div>
</div>


<?php include_once('../../inc/footer.php'); ?>